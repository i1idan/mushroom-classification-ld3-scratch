# -*- coding: utf-8 -*-
"""Hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sygHqvk0q2joBLtaOA3NQILrUbrA54yI

**Part A**

**Load Data**
"""

# import necessary libraries
 
import pandas as pd
 
df = pd.read_csv('/content/drive/MyDrive/tree/Agaricus-lepiota.data.txt', header=None)
df
 
#  Specify Column Names (Features) 
 
df.columns = ["class", "cap-shape", "cap-surface", "cap-color", "bruises",
             "odor", "gill-attachment", "gill-spacing", "gill-size",
             "gill-color", "stalk-shape", "stalk-root",
             "stalk-surface-above-ring",
             "stalk-surface-below-ring",
             "stalk-color-above-ring",
             "stalk-color-below-ring",
             "veil-type", "veil-color", "ring-number", "ring-type",
             "spore-print-color", "population", "habitat" ]
 
df

"""Missing Values"""

df.isnull().sum()

df.isnull().any() # This code shows that there is a column contains a null value or not, as you can see below the dataset has a columns contains "nan" value.
# So, this code returns true for stalk-root column.

# Also, we can write a simple for loop to see column(s) which contains(s) non-sense or null value(s).
 
for i in df.columns:
    print(i,df[i].unique())

""" as you can see 'stalk-root' contains '?' value which is non-sense. We need to fill it with reasonable value instead of '?'. There are several ways to fill missing or non-sense data.This method is very dummy, but sometimes it may be working well suprisingly. The column(s) contain(s) missing or non-sense value(s) can be wiped out. </br>If the column contains missing or non-sense value is categorical, then we can fill the missing or non-sense values with value has most frequency in that column. If the column is numerical, then we can fill the missing or non-sense values with average or median value of that column. </br>We can built decision tree or regression model for missing or non-sense value(s)."""

df['stalk-root'].value_counts()

"""For edible class"""

temp = df.loc[df['class'] == 'e' ]
temp['stalk-root'].value_counts()

"""For poisonous class"""

temp = df.loc[df['class'] == 'p' ]
temp['stalk-root'].value_counts()

df['stalk-root'] = df['stalk-root'].replace('?','b') # '?' in stalk-root attribute filled with 'b' has most frequency.
# Let's see whether non-sense data exists, or not.
df['stalk-root'].value_counts()

temp = df.drop('class', axis=1)
temp['class']= df['class']
df = temp
df

print('Number of samples: ', df.shape[0])
print('Number of attributes: ', df.shape[1])
 
value_counts = df['class'].value_counts()
e = value_counts['e']
p = value_counts['p']
 
print('\nEdible:    ', e)
print('Poisonous: ', p)
print('\nTotal:     ', e + p)

"""
**ID3 Algorithm**

**In the ID3 algorithm, decision trees are** **calculated using the concept of entropy and** **information gain.**
**Entropy can be defined as:**
**H(S)=−∑i=1Npilog2pi **"""

import pandas as pd
import numpy as np
 
# eps for making value a bit greater than 0 later on
eps = np.finfo(float).eps
 
from numpy import log2 as log

def find_entropy(df):
    '''
    Function to calculate the entropy of the label class
    '''
    Class = df.keys()[-1] 
    entropy = 0
    values = df[Class].unique()
    for value in values:
        fraction = df[Class].value_counts()[value]/len(df[Class])
        entropy += -fraction*np.log2(fraction)
    return entropy

def find_entropy_attribute(df,attribute):
    '''
    Function to calculate the entropy of all features.
    '''
    Class = df.keys()[-1]   
    target_variables = df[Class].unique()  
    variables = df[attribute].unique()
    entropy2 = 0
    for variable in variables:
        entropy = 0
        for target_variable in target_variables:
                num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])
                den = len(df[attribute][df[attribute]==variable])
                fraction = num/(den+eps)
                entropy += -fraction*log(fraction+eps)
        fraction2 = den/len(df)
        entropy2 += -fraction2*entropy
    return abs(entropy2)

def find_winner(df):
    '''
    Function to find the feature with the highest information gain.
    '''
    Entropy_att = []
    IG = []
    for key in df.keys()[:-1]:
#         Entropy_att.append(find_entropy_attribute(df,key))
        IG.append(find_entropy(df)-find_entropy_attribute(df,key))
    return df.keys()[:-1][np.argmax(IG)]

def get_subtable(df, node, value):
    '''
    Function to get a subtable of met conditions.
    
    node: Column name
    value: Unique value of the column
    '''
    return df[df[node] == value].reset_index(drop=True)

def buildTree(df,tree=None): 
    '''
    Function to build the ID3 Decision Tree.
    '''
    Class = df.keys()[-1]  
    #Here we build our decision tree
 
    #Get attribute with maximum information gain
    node = find_winner(df)
    
    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values
    attValue = np.unique(df[node])
    
    #Create an empty dictionary to create tree    
    if tree is None:                    
        tree={}
        tree[node] = {}
    
   #We make loop to construct a tree by calling this function recursively. 
    #In this we check if the subset is pure and stops if it is pure. 
 
    for value in attValue:
        
        subtable = get_subtable(df,node,value)
        clValue,counts = np.unique(subtable['class'],return_counts=True)                        
        
        if len(counts)==1:#Checking purity of subset
            tree[node][value] = clValue[0]                                                    
        else:        
            tree[node][value] = buildTree(subtable) #Calling the function recursively 
                   
    return tree

def predict(inst,tree):
    '''
    Function to predict for any input variable.
    '''
    #Recursively we go through the tree that we built earlier
 
    for nodes in tree.keys():  
        
        value = inst[nodes]
        tree = tree[nodes][value]
        prediction = 0
            
        if type(tree) is dict:
            prediction = predict(inst, tree)
        else:
            prediction = tree
            break;                            
        
    return prediction

""" Data Transformation
 
Our dataset is full of categorical data. We need to convert these categorical values into numerical representatives. There are two ways to do this.
Label Encoding: This method can be perform on columns with binary values. We can't apply this method on a columns has more than two different values, because this method may cause a hierarchy between values in a column. So, this may cause incorrect classification.
 
One-Hot Encoding: This method can also be called "Dummy Variable" by some. In this method, each distinct values in a column transform into a column. Unlike label encoding, there is no trouble in terms of hierarchy. As dataset grows dimensionally, this method may cause low running performance.
"""

from sklearn.preprocessing import LabelEncoder
labelencoder=LabelEncoder()
for col in df.columns:
    df[col] = labelencoder.fit_transform(df[col])
 
df.head()

"""**10 Fold Cross Validation**"""

import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
 
 
kf = KFold(n_splits=10)
 
X = df
y = df['class']
 
accuracy_list = []
 
for train_index, test_index in kf.split(X):
  #  print("TRAIN:", train_index, "TEST:", test_index)
   X_train, X_test = X.iloc[train_index], X.iloc[test_index]
   X_test.drop('class', axis=1)
   y_train, y_test = y.iloc[train_index], y.iloc[test_index]
   tree = buildTree(X_train)
   
   y_pred=[]
   for i in range(len(X_test)):
     inst = pd.Series(X_test.iloc[i])
     prediction = predict(inst, tree)
     y_pred.append(prediction)
   accuracy =  accuracy_score(y_test, y_pred)
   accuracy_list.append(accuracy)
   report = classification_report(y_test, y_pred)
   print('Fold Number:', len(accuracy_list))  
   print('Accuracy Score:', accuracy)
   print('Classification Report:', '\n', report)  
   print(100 * '=')
 
print('\n')
print('Final Accuracy', np.mean(accuracy_list))
print('Standard Deviation', np.std(accuracy_list))
print('\n')
print(100 * '=')